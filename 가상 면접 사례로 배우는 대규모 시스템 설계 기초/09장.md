# 9장 웹크롤러 설계

웹크롤러는 몇 개의 웹 페이지에서 시작해서 링크를 따라 나가면서 새로운 컨텐츠를 수집한다

**용도**
- 검색 엔진 인덱싱: 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다
- 웹 아카이빙: 장기보관하기 위해 웹에서 정보를 모은다
- 웹 마이닝: 인터넷에서 유용한 정보를 추출할 수 있다
- 웹 모니터링: 저작권이나 상표권이 침해되는 사례 모니터링 가능

## 1단계: 문제 이해 및 설계 범위 확정

웹 크롤러의 기본 알고리즘
- URL 집합이 입력으로 주어지면, 해당 URL이 가리키는 모든 웹 페이지 다운로드
- 다운받은 웹페이지에서 URL 추출
- 추출된 URL을 다운로드할 URL 목록에 추가하고 처음부터 반복

**던질만한 질문들**
- 크롤러의 용도는 무엇인지
- 매달 얼마나 많은 웹페이지 수집해야 하는지
- 수집한 페이지를 저장해야 하는지
- 중복된 페이지 처리는?

**웹크롤러가 만족하면 좋을 하는 속성들**
- 규모 확장성: parallel하게 동작하도록 설정
- 안정성: 잘못 작성된 HTML, 반응 없는 서버, 장애, 악성코드 붙어있는 링크
- 예절: 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안 된다
- 확장성: 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다 (이미지, pdf)

### 개략적 규모 측정

(이런 계산 과정이 엄청나다...)
- 매달 10억개의 웹페이지를 다운로드한다
- QPS = 10억 / 30일 / 24시간 / 3600초 = 400페이지/초
- 최대 QPS = 2 * QPS = 800
- 웹페이지의 평균 크기는 500K라고 가정
- 10억 페이지 * 500K = 500TB/월
- 5년간 보관 = 500TB * 12 * 5 = 30PB

## 2단계: 개략적인 설계안 제시 및 동의 구하기

- 시작 URL 집합: 시작 URL로부터 출발한다
- 미수집 URL 저장소: 다운로드 할 URL 저장소
- HTML 다운로더: 웹 페이지를 다운로드한다
- 도메인 이름 변환기: URL을 IP주소로 변환한다
- 콘텐츠 파서: 파싱해서 검증하는 과정을 거쳐야 한다, 크롤링 서버 안에 두면 느려지므로 독립된 컴포넌트로 관리
- 중복 컨텐츠 판별: 29% 가량은 중복 웹 페이지, 웹페이지 해시 값으로 쉽게 중복 판별 가능
- 콘텐츠 저장소: HTML 문서를 보관
- URL 추출기: HTML 페이지를 파싱해서 링크를 골라낸다, 상대경로를 절대 경로로 변환
- URL 필터: 특정 콘텐츠 타입, 접속시 오류가 발생하는 URL, 접근 제외 목록 URL 크롤링 대상 배제
- 이미 방문한 URL 판별: 블룸 필터, 해시 테이블 활용해서 판별
- URL 저장소: 이미 방문한 URL 저장

## 3단계: 상세 설계

### 웹 방문 알고리즘

- DFS: 그래프 크기가 클 경우 어느정도로 깊이 갈지 가늠 안 됨
- BFS를 사용하는 것이 낫다
- 한 페이지에서 나오는 링크의 대부분은 같은 서버로 되돌아간다
- wiki 페이지의 링크는 wiki 페이지일 경우가 높다
- BFS는 URL 간의 우선순위를 두지 않으므로 같은 호스트로 요청을 많이 보낼 수 있다
- 우선순위를 두는 것이 좋다

### 미수집 URL 저장소

- 예의를 갖추자
- 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청
- 여러개의 큐를 둬서 호스트별로 큐를 저장
- 같은 호스트에 속하는 URL은 같은 큐로
- 큐 선택기에서 큐를 순환하면서 URL을 꺼낸 뒤 작업 스레드에 전달한다
- 우선순위를 두고 싶다면 순위 결정 장치를 둬서 우선순위에 따라 큐에 저장
- 우선순위가 높은 큐를 우선으로 URL을 꺼낸다

### HTML 다운로더

- HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려받는다
- Robots.txt
	- 웹사이트가 크롤러와 소통하는 표준 방식
	- Robots.txt 다운받아 캐시에 보관
	- Robots.txt 내용에 따라 다운 받을 수 없는 경로 판단
- 성능 최적화
	- 여러 서버에서 분산 크롤링
	- DNS 응답을 캐싱해놓고 크론잡으로 주기적으로 업데이트
	- 크롤링 서버와 지역적으로 가까운게 시간 감소
	- 응답하지 않거나 느린 웹서버 -> 타임아웃 설정
- 안정성
	- 부하 분산시 안정 해시 적용
	- 크롤링 상태 및 수집 데이터 디스크에 저장 (중단되더라도 복구 가능하게)
	- 예외처리 (예외가 나더라도 중단되면 안 된다)
	- 데이터 검증
- 문제 있는 콘텐츠 감지 및 회피
	- 중복 콘텐츠: 웹의 30% 가량은 중복, 해시나 체크섬으로 탐지
	- 거미 덫: 무한히 깊은 디렉터리에 빠짐
	- 데이터 노이즈: 광고나 스팸 URL
