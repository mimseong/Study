# 9장 웹크롤러 설계

웹크롤러는 몇 개의 웹 페이지에서 시작해서 링크를 따라 나가면서 새로운 컨텐츠를 수집한다

**용도**
- 검색 엔진 인덱싱: 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다
- 웹 아카이빙: 장기보관하기 위해 웹에서 정보를 모은다
- 웹 마이닝: 인터넷에서 유용한 정보를 추출할 수 있다
- 웹 모니터링: 저작권이나 상표권이 침해되는 사례 모니터링 가능

## 1단계: 문제 이해 및 설계 범위 확정

웹 크롤러의 기본 알고리즘
- URL 집합이 입력으로 주어지면, 해당 URL이 가리키는 모든 웹 페이지 다운로드
- 다운받은 웹페이지에서 URL 추출
- 추출된 URL을 다운로드할 URL 목록에 추가하고 처음부터 반복

**던질만한 질문들**
- 크롤러의 용도는 무엇인지
- 매달 얼마나 많은 웹페이지 수집해야 하는지
- 수집한 페이지를 저장해야 하는지
- 중복된 페이지 처리는?

**웹크롤러가 만족하면 좋을 하는 속성들**
- 규모 확장성: parallel하게 동작하도록 설정
- 안정성: 잘못 작성된 HTML, 반응 없는 서버, 장애, 악성코드 붙어있는 링크
- 예절: 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안 된다
- 확장성: 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다 (이미지, pdf)

### 개략적 규모 측정

(이런 계산 과정이 엄청나다...)
- 매달 10억개의 웹페이지를 다운로드한다
- QPS = 10억 / 30일 / 24시간 / 3600초 = 400페이지/초
- 최대 QPS = 2 * QPS = 800
- 웹페이지의 평균 크기는 500K라고 가정
- 10억 페이지 * 500K = 500TB/월
- 5년간 보관 = 500TB * 12 * 5 = 30PB

## 2단계: 개략적인 설계안 제시 및 동의 구하기

- 시작 URL로부터 출발한다
  
